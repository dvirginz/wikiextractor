{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37764bitdocbertconda2fac149f98cd481e9df4b3c2ae7d85a5",
   "display_name": "Python 3.7.7 64-bit ('docBert': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "ERROR:root:args.bytes[-1]: 2k\n"
    },
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_wiki_dump.bz2'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\t-dvginz\\PycharmProjects\\docBert\\data\\wikiextractor\\WikiExtractor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   3308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3309\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3310\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\t-dvginz\\PycharmProjects\\docBert\\data\\wikiextractor\\WikiExtractor.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   3294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3295\u001b[0m     process_dump(input_file, args.templates, output_path, file_size,\n\u001b[1;32m-> 3296\u001b[1;33m                  args.compress, args.processes)\n\u001b[0m\u001b[0;32m   3297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3298\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreateLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquiet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\t-dvginz\\PycharmProjects\\docBert\\data\\wikiextractor\\WikiExtractor.py\u001b[0m in \u001b[0;36mprocess_dump\u001b[1;34m(input_file, template_file, out_file, file_size, file_compress, process_count)\u001b[0m\n\u001b[0;32m   2880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2881\u001b[0m     \u001b[1;31m# collect siteinfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2882\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2883\u001b[0m         \u001b[1;31m# When an input file is .bz2 or .gz, line can be a bytes even in Python 3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\docBert\\lib\\fileinput.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_readline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filelineno\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\docBert\\lib\\fileinput.py\u001b[0m in \u001b[0;36m_readline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    360\u001b[0m                 \u001b[1;31m# This may raise OSError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_openhook\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_openhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\docBert\\lib\\fileinput.py\u001b[0m in \u001b[0;36mhook_compressed\u001b[1;34m(filename, mode)\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.bz2'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mbz2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBZ2File\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\docBert\\lib\\bz2.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, buffering, compresslevel)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_wiki_dump.bz2'"
     ]
    }
   ],
   "source": [
    "## Create the wikipedia dataset\n",
    "## changes have been made to WikiExtractor.py to be suitable for our requirements (no sub level paragraphs, one article per file)\n",
    "%run -i WikiExtractor.py path_to_wiki_dump.bz2 -o .//etracted --min_text_length 2048 --sections --sections_top_level --filter_disambig_pages --processes 8 --bytes 2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class TextParser(object):\n",
    "        def __init__ (self, id_string=\"id\",category_string=\"categories\",title_string=\"title\",**kwargs):\n",
    "            EQUAL_GROUP = r'\\=\\\"(.*?)\\\"'\n",
    "            self.id_string= \"{}{}\".format(id_string,EQUAL_GROUP)\n",
    "            self.category_string= \"{}{}\".format(category_string,EQUAL_GROUP)\n",
    "            self.title_string= \"{}{}\".format(title_string,EQUAL_GROUP)\n",
    "        \n",
    "        def extract_title_id_categories(self,path,attribiutes_line):\n",
    "            id_search = re.search(self.id_string, attribiutes_line, re.IGNORECASE)\n",
    "            if id_search:\n",
    "                id = id_search.group(1)\n",
    "\n",
    "            title_search = re.search(self.title_string, attribiutes_line, re.IGNORECASE)\n",
    "            if title_search:\n",
    "                title = title_search.group(1)\n",
    "\n",
    "            category_search = re.search(self.category_string, attribiutes_line, re.IGNORECASE)\n",
    "            if category_search:\n",
    "                categories = category_search.group(1).split(\", \")\n",
    "\n",
    "            try:\n",
    "                return id,title,categories\n",
    "            except:\n",
    "                print(\"one of {}|{}|{} was missing in path {}\".format(self.id_string,self.category_string,self.title_string,path))\n",
    "\n",
    "        def __call__(self,path):\n",
    "            # Using readlines() \n",
    "            article = open(path, 'r', encoding='utf-8') \n",
    "            lines = article.readlines() \n",
    "            if(len(lines) < 2):\n",
    "                print(\"File: {} is empty\".format(path))\n",
    "                return -1,0,0\n",
    "\n",
    "            attribiutes_line = lines.pop(0) # first line is attribiutes\n",
    "            id,title,categories = self.extract_title_id_categories(path,attribiutes_line)\n",
    "            article_name = lines.pop(0) \n",
    "\n",
    "            ##TODO split paragraph-name tuples\n",
    "            return id,title,categories\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "File: C:\\Users\\t-dvginz\\PycharmProjects\\docBert\\data\\wikiextractor\\test_folder\\AA\\wiki_00 is empty\n{\n    \"A\": 4,\n    \"Achilles\": 6,\n    \"Alabama\": 5,\n    \"Albedo\": 3,\n    \"Anarchism\": 1,\n    \"Autism\": 2\n}\n{\n    \"1\": {\n        \"categories\": [\n            \"Anti-fascism\",\n            \"Anti-capitalism\",\n            \"Political ideologies\",\n            \"Political culture\",\n            \"Far-left politics\",\n            \"Anarchism\",\n            \"Political movements\",\n            \"Economic ideologies\",\n            \"Libertarianism\",\n            \"Libertarian socialism\"\n        ],\n        \"id\": \"12\",\n        \"path\": \"C:\\\\Users\\\\t-dvginz\\\\PycharmProjects\\\\docBert\\\\data\\\\wikiextractor\\\\test_folder\\\\AA\\\\wiki_01\",\n        \"title\": \"Anarchism\"\n    },\n    \"2\": {\n        \"categories\": [\n            \"Neurological disorders in children\",\n            \"Autism\",\n            \"Pervasive developmental disorders\",\n            \"Psychiatric diagnosis\",\n            \"Neurological disorders\",\n            \"Communication disorders\",\n            \"Mental and behavioural disorders\",\n            \"RTT\",\n            \"Articles containing video clips\"\n        ],\n        \"id\": \"25\",\n        \"path\": \"C:\\\\Users\\\\t-dvginz\\\\PycharmProjects\\\\docBert\\\\data\\\\wikiextractor\\\\test_folder\\\\AA\\\\wiki_02\",\n        \"title\": \"Autism\"\n    },\n    \"3\": {\n        \"categories\": [\n            \"Electromagnetic radiation\",\n            \"Scattering\",\n            \"absorption and radiative transfer (optics)\",\n            \"Climatology\",\n            \"Land surface effects on climate\",\n            \"Climate feedbacks\",\n            \"Climate forcing\",\n            \"Radiometry\",\n            \"Radiation\"\n        ],\n        \"id\": \"39\",\n        \"path\": \"C:\\\\Users\\\\t-dvginz\\\\PycharmProjects\\\\docBert\\\\data\\\\wikiextractor\\\\test_folder\\\\AA\\\\wiki_03\",\n        \"title\": \"Albedo\"\n    },\n    \"4\": {\n        \"categories\": [\n            \"ISO basic Latin letters\"\n        ],\n        \"id\": \"290\",\n        \"path\": \"C:\\\\Users\\\\t-dvginz\\\\PycharmProjects\\\\docBert\\\\data\\\\wikiextractor\\\\test_folder\\\\AA\\\\wiki_04\",\n        \"title\": \"A\"\n    },\n    \"5\": {\n        \"categories\": [\n            \"1819 establishments in the United States\",\n            \"States of the Gulf Coast of the United States\",\n            \"States of the Confederate States\",\n            \"States of the United States\",\n            \"States and territories established in 1819\",\n            \"Alabama\",\n            \"Southern United States\"\n        ],\n        \"id\": \"303\",\n        \"path\": \"C:\\\\Users\\\\t-dvginz\\\\PycharmProjects\\\\docBert\\\\data\\\\wikiextractor\\\\test_folder\\\\AA\\\\wiki_05\",\n        \"title\": \"Alabama\"\n    },\n    \"6\": {\n        \"categories\": [\n            \"Characters in the Iliad\",\n            \"Achaean Leaders\",\n            \"Thessalians in the Trojan War\",\n            \"Greek mythological heroes\",\n            \"Demigods of Classical mythology\",\n            \"Characters in Greek mythology\",\n            \"People of the Trojan War\",\n            \"Fictional LGBT characters in literature\",\n            \"LGBT themes in Greek mythology\",\n            \"Kings of the Myrmidons\"\n        ],\n        \"id\": \"305\",\n        \"path\": \"C:\\\\Users\\\\t-dvginz\\\\PycharmProjects\\\\docBert\\\\data\\\\wikiextractor\\\\test_folder\\\\AA\\\\wiki_06\",\n        \"title\": \"Achilles\"\n    }\n}\n"
    }
   ],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "\n",
    "##TODO############\n",
    "##Make sure no more that one doc in page\n",
    "##add wget of wiki dump\n",
    "##Download in server\n",
    "\n",
    "####################\n",
    "\n",
    "## Create title-doc_id dict and doc_id-[path] dict\n",
    "PATH_TO_PARSED_WIKI = \"C:\\\\Users\\\\t-dvginz\\\\PycharmProjects\\\\docBert\\\\data\\\\wikiextractor\\\\test_folder\"\n",
    "\n",
    "regex = PATH_TO_PARSED_WIKI + '\\\\**\\\\*[0-9][0-9]'\n",
    "\n",
    "## Need running index for data loader\n",
    "text_parser = TextParser()\n",
    "name_to_running_id_dict = {}\n",
    "running_id_to_path_title_categories = {}\n",
    "idx=0\n",
    "for idx_parse,wiki_f in enumerate(glob.iglob(regex, recursive=True)):      #iglob because too big for memory\n",
    "    id,title,categories = text_parser(wiki_f)\n",
    "    if(id==-1):continue\n",
    "    name_to_running_id_dict[title] = idx\n",
    "    running_id_to_path_title_categories[idx] = {\"title\":title,\"path\":wiki_f,\"categories\":categories,\"id\":id}\n",
    "    \n",
    "    if idx > 5: break\n",
    "    idx +=1\n",
    "\n",
    "    if(idx_parse % 10000 == 0): print(\"parsed {} articles\".format(idx_parse))\n",
    "\n",
    "print(json.dumps(name_to_running_id_dict,sort_keys=True, indent=4))\n",
    "print(json.dumps(running_id_to_path_title_categories,sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}